{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "lab11_solutions.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4SfaKzDfXofr",
        "colab_type": "text"
      },
      "source": [
        "# Introduction\n",
        "\n",
        "In this lab we explore approaches for reducing bias in word embeddings.  This lab is closely based on the article \"Man is to Computer Programmer as Woman is to Homemaker? Debiasing Word Embeddings\" by Bolukbasi et al., and is adapted from a lab in the Deep Learning course on Coursera."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wEM_E4b8ETux",
        "colab_type": "text"
      },
      "source": [
        "### Module imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wQl3qC2bXlhu",
        "colab_type": "code",
        "outputId": "2d68359e-52d5-42fa-d0be-3fdddecc1bdd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "%tensorflow_version 1.x\n",
        "\n",
        "from keras import layers\n",
        "from keras import models\n",
        "from keras import optimizers\n",
        "import os\n",
        "import time\n",
        "from google.colab import drive\n",
        "import matplotlib.pyplot as plt\n",
        "import pickle\n",
        "import math\n",
        "import numpy as np"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TensorFlow 1.x selected.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bo-bhEnQcEuI",
        "colab_type": "text"
      },
      "source": [
        "## Set up -- getting the data and word embeddings\n",
        "I have shared the necessary files with you in the same Google drive folder we used for lab 9.  To get the data into colab, run the code cell below and click on the link that is displayed.  It will pop up a new browser tab where you have to authorize Colab to access your google drive.  Then, copy the sequence of numbers and letters that is displayed and paste it in the space that shows up in the code cell below.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3lw9KLykcaci",
        "colab_type": "code",
        "outputId": "0071ddb0-5d96-4f64-87a5-95ca1d085bea",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        }
      },
      "source": [
        "drive.mount('/content/drive')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9XQqHbnEhS9k",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 167
        },
        "outputId": "2ec32ac0-a3fe-442e-a94e-f266276c53d4"
      },
      "source": [
        "os.mkdir(\"/content/stat344ne_glove/\")"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileExistsError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileExistsError\u001b[0m                           Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-f4a41bf3b700>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmkdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/content/stat344ne_glove/\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mFileExistsError\u001b[0m: [Errno 17] File exists: '/content/stat344ne_glove/'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "79C3EFu5cdyF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!unzip -uq \"/content/drive/My Drive/stat344ne_imdb/glove.6B.50d.txt.zip\" -d \"/content/stat344ne_glove/glove/\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G-tcVwlOkOs-",
        "colab_type": "text"
      },
      "source": [
        "### Load word embeddings\n",
        "\n",
        "We are working here with the GloVe (**Glo**bal **Ve**ctors for word representation) word embeddings.  The code for loading them is the same as in labs 9 and 10."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZzDJPuishPVV",
        "colab_type": "code",
        "outputId": "8e0d164a-ff1f-458d-b49c-f543ed02084b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "glove_dir = \"/content/stat344ne_glove/glove\"\n",
        "\n",
        "embeddings_index = {}\n",
        "f = open(os.path.join(glove_dir, 'glove.6B.50d.txt'))\n",
        "for line in f:\n",
        "    values = line.split()\n",
        "    word = values[0]\n",
        "    coefs = np.asarray(values[1:], dtype='float32')\n",
        "    embeddings_index[word] = coefs\n",
        "f.close()\n",
        "\n",
        "print('Found %s word vectors.' % len(embeddings_index))"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 400000 word vectors.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8cCqmkgOWMGa",
        "colab_type": "text"
      },
      "source": [
        "### Cosine Similarity\n",
        "\n",
        "We're going to explore how bias shows up in word embeddings as evidenced by analogies.  Here's a utility function to calculate cosine similarity, from lab 9."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bYi7-ES2M4T9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def cos_similarity(v, w):\n",
        "  '''\n",
        "  Calculate cosine similarity of vectors v and w\n",
        "\n",
        "  Arguments:\n",
        "   - v: column vector of shape (d, 1)\n",
        "   - w: column vector of shape (d, 1)\n",
        "  \n",
        "  Return:\n",
        "   - cosine similarity of v and w\n",
        "  '''\n",
        "  # add your calculation here.  You can add more lines if it's helpful\n",
        "  result = np.dot(v.T, w) / (np.linalg.norm(v) * np.linalg.norm(w))\n",
        "  return(result)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q-wKygUaYKpN",
        "colab_type": "text"
      },
      "source": [
        "Based on this function, we found analogies by examining the cosine similarity of differences between pairs of words.  For example, the analogy \"paris is to france as rome is to italy\" is represented by the fact that the cosine similarity of the difference $(e_{paris} - e_{rome})$ and the difference $(e_{rome} - e_{italy})$ is large:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cZRCunfCM-hD",
        "colab_type": "code",
        "outputId": "10c717cc-07f0-46a4-9dcc-5eddc6733881",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "cos_similarity(\n",
        "    embeddings_index.get('paris') - embeddings_index.get('france'),\n",
        "    embeddings_index.get('rome') - embeddings_index.get('italy')\n",
        ")"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.67514807"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0SYT50BjZSFm",
        "colab_type": "text"
      },
      "source": [
        "#### 1. Function to complete analogies\n",
        "\n",
        "Suppose we want to find analogies.  For example, what word completes the analogy \"Paris is to France as Sydney is to \\_\\_\\_\\_\\_\"?  More generally, we will seek the best word to fill in the blank in the analogy \"\\<`word_a`\\> is to \\<`word_b`\\> as \\<`word_c`\\> is to \\_\\_\\_\\_\\_\".\n",
        "\n",
        "One way to find an answer is with the following algorithm:\n",
        "\n",
        "1. Initialize `best_word = None` and `best_score = -100` (any number less than -1 would do)\n",
        "2. For each `word_d` in the `embeddings_index`,\n",
        "\n",
        "  a. If the `word_d` is one of `word_a`, `word_b`, or `word_c`, `continue` (skip the rest of this iteration of the loop; we don't want to complete the analogy using one of the input words.)\n",
        "\n",
        "  b. Compute `current_score` as the cosine similarity of $(e_{<word\\_a>} - e_{<word\\_b>})$ and $(e_{<word_c>} - e_{<word\\_d>})$\n",
        "\n",
        "  c. If the `current_score` is greater than `best_score`, set `best_word = word_d` and `best_score = current_score`.\n",
        "\n",
        "3. Return `best_word`\n",
        "\n",
        "Fill in the missing steps in the `complete_analogy` function below to implement this algorithm:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PZW7z-MDObGP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def complete_analogy(word_a, word_b, word_c, embeddings_index):\n",
        "    \"\"\"\n",
        "    Performs the word analogy task as explained above: a is to b as c is to ____. \n",
        "    \n",
        "    Arguments:\n",
        "     - word_a: a word, string\n",
        "     - word_b: a word, string\n",
        "     - word_c: a word, string\n",
        "     - embeddings_index: dictionary that maps words to their corresponding vectors. \n",
        "    \n",
        "    Returns:\n",
        "    best_word: the word such that e_a - e_b is close to e_d - e_best_word, as\n",
        "      measured by cosine similarity\n",
        "    \"\"\"\n",
        "    \n",
        "    # convert words to lowercase\n",
        "    word_a, word_b, word_c = word_a.lower(), word_b.lower(), word_c.lower()\n",
        "    \n",
        "    # Get the word embeddings e_a, e_b and e_c (â‰ˆ1-3 lines)\n",
        "    e_a = embeddings_index.get(word_a)\n",
        "    e_b = embeddings_index.get(word_b)\n",
        "    e_c = embeddings_index.get(word_c)\n",
        "    \n",
        "    # Initialize\n",
        "    words = embeddings_index.keys() # all possible words in the embeddings index\n",
        "    best_score = -100               # Initialize best_score to a large negative number\n",
        "    best_word = None                # Initialize best_word to None\n",
        "\n",
        "    # loop over the whole word vector set\n",
        "    for word_d in words:        \n",
        "        # to avoid best_word being one of the input words, skip the input words\n",
        "        # there are many ways to do this; the simplest is to compare word_d to\n",
        "        # each of the other words one at a time with ==, and check all three\n",
        "        # using or.\n",
        "        if word_d == word_a or word_d == word_b or word_d == word_c:\n",
        "            continue\n",
        "        \n",
        "        # Find the embedding for word_d\n",
        "        e_d = embeddings_index.get(word_d)\n",
        "\n",
        "        # Compute cosine similarity between the vector (e_b - e_a) and the\n",
        "        # vector e_c - e_d\n",
        "        current_score = cos_similarity(e_a - e_b, e_c - e_d)\n",
        "        \n",
        "        # If the current_score is more than the best_score seen so far,\n",
        "        # update the best score and best word\n",
        "        if current_score > best_score:\n",
        "            best_score = current_score\n",
        "            best_word = word_d\n",
        "    \n",
        "    return best_word"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8vrSynssAyTZ",
        "colab_type": "text"
      },
      "source": [
        "#### 2. Analogy for locations\n",
        "Use your function to complete the analogy \"Paris is to France as Sydney is to \\_\\_\\_\\_\\_.\""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g8H8Kj94V4I_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "60e817cd-0988-41b2-ffd2-f8cd1ba4b82e"
      },
      "source": [
        "complete_analogy('paris', 'france', 'sydney', embeddings_index)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'australia'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D6bJRmdhBQ7K",
        "colab_type": "text"
      },
      "source": [
        "#### 3. Analogy for professions\n",
        "Let's look at the problematic example of gendered professions we saw in the lecture: use your function to complete the analogy \"man is to doctor as woman is to \\_\\_\\_\\_\\_.\"  Also compute the cosine similarity for the word embedding differences in this analogy."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NmdyxhJ5OkwX",
        "colab_type": "code",
        "outputId": "b7997dde-f300-48fd-cfd9-80e49ab9fbb8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "# complete the analogy\n",
        "complete_analogy('man', 'doctor', 'woman', embeddings_index)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'nurse'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xM82u8qAOISm",
        "colab_type": "code",
        "outputId": "1dd786a2-afa7-4485-eef5-30fe9e1d317c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "# find the cosine similarity for the word embedding differences in\n",
        "# the completed analogy.\n",
        "cos_similarity(\n",
        "    embeddings_index.get('man') - embeddings_index.get('doctor'),\n",
        "    embeddings_index.get('woman') - embeddings_index.get('nurse')\n",
        ")"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.6831788"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bIQGo2x5OQUy",
        "colab_type": "text"
      },
      "source": [
        "## A couple of useful features in Python\n",
        "For the next problem, I want to encourage you to use two useful features in python: list comprehensions, and simultaneous loops over multiple vectors with `zip`.  Here is a brief introduction to those two features:\n",
        "\n",
        "### List comprehensions\n",
        "\n",
        "A list comprehension is basically a way to easily create a list with a for loop.  Here's an example of creating a list of the squares of integers from 0 to 9."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h51_vIv0GI4R",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "efb4bcaa-adc9-47c9-f9ef-81a9a61a5177"
      },
      "source": [
        "# Method 1: what you might usually do\n",
        "\n",
        "# create an empty list\n",
        "result1 = []\n",
        "\n",
        "# in a for loop, append the results one at a time\n",
        "for i in range(10):\n",
        "  result1.append(i**2)\n",
        "\n",
        "result1"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0, 1, 4, 9, 16, 25, 36, 49, 64, 81]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DeTWURLYKgBu",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "b1abede7-325a-4399-b225-47ad4e8e57c9"
      },
      "source": [
        "# Method 2: using a list comprehension\n",
        "# the format is [<quantity for one entry of the list> for <loop specification>]\n",
        "result2 = [i**2 for i in range(10)]\n",
        "result2"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0, 1, 4, 9, 16, 25, 36, 49, 64, 81]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PKm0nZjNPJ96",
        "colab_type": "text"
      },
      "source": [
        "### Iterating over multiple lists with `zip`\n",
        "\n",
        "Suppose I have two lists of length 5, and I want to write a for loop that processes the corresponding entries.  For example, the following code concatenates corresponding entries of two lists and prints the results out:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s-UYPmTWPju-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        },
        "outputId": "4834b792-a9b9-4f2d-a696-b1ff7e758354"
      },
      "source": [
        "list1 = ['a', 'b', 'c', 'd', 'e']\n",
        "list2 = ['v', 'w', 'x', 'y', 'z']\n",
        "\n",
        "for i in range(5):\n",
        "  print(list1[i] + \"_\" + list2[i])"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "a_v\n",
            "b_w\n",
            "c_x\n",
            "d_y\n",
            "e_z\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "66jCpQpbQAga",
        "colab_type": "text"
      },
      "source": [
        "The function zip lets us do this without introducing the new variable i, by directly referencing the letters we want to work with.  This doesn't necessarily make your code shorter, but it can make it more readable and easier to understand.  Behind the scenes, the way that this works is that zip creates a generator that generates tuples of corresponding values from its arguments."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iOqDRU2gP-0g",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        },
        "outputId": "b4d290d3-19e7-4b65-bba8-fe9fadead4ce"
      },
      "source": [
        "list1 = ['a', 'b', 'c', 'd', 'e']\n",
        "list2 = ['v', 'w', 'x', 'y', 'z']\n",
        "\n",
        "for letter1, letter2 in zip(list1, list2):\n",
        "  print(letter1 + \"_\" + letter2)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "a_v\n",
            "b_w\n",
            "c_x\n",
            "d_y\n",
            "e_z\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7RtPquNlQnev",
        "colab_type": "text"
      },
      "source": [
        "### Putting together list comprehensions and zip\n",
        "These ideas can also be combined, as in the following example:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yw-5uLFdQxaU",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "815ac35b-13f7-414f-d83c-e94c1d5c041a"
      },
      "source": [
        "list1 = ['a', 'b', 'c', 'd', 'e']\n",
        "list2 = ['v', 'w', 'x', 'y', 'z']\n",
        "\n",
        "# use a list comprehension combined with zip to iterate over the two lists at once\n",
        "result  = [letter1 + \"_\" + letter2 for letter1, letter2 in zip(list1, list2)]\n",
        "result"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['a_v', 'b_w', 'c_x', 'd_y', 'e_z']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AjJ2UZNABtus",
        "colab_type": "text"
      },
      "source": [
        "#### 4. Identify a \"gender direction\" in the word embedding space.\n",
        "\n",
        "In the original article, this was done by:\n",
        "\n",
        "1. Finding the differences in embeddings between many gendered word pairs such as \"woman\" and \"man\", \"mother\" and \"father\", and so on.\n",
        "2. Conducting Principal Component Analysis (PCA) to find a single direction in the word embedding space that captures most of the variability in these differences in word pairs.\n",
        "\n",
        "Not everyone in this class has seen PCA, so we will simplify this second step by taking the mean instead; this will work nearly as well.  Here's our version of this procedure:\n",
        "\n",
        "1. Create a list of the differences in embeddings between multiple gendered word pairs such as \"woman\" and \"man\", \"mother\" and \"father\", and so on.\n",
        "2. Use `np.mean()` to compute the average across all word pair differences in the list.\n",
        "\n",
        "For step 1, we will use the word pairs used in the article by Bolukbasi et al. (Figure 2), other than the pair 'Mary' and 'John', which are not both included in the GloVe word embedding.  These are defined in the arrays `female_words` and `male_words` below."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZI2_g2tvMV4c",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 212
        },
        "outputId": "4c18ad6e-87f1-497a-e962-7633d0b7e236"
      },
      "source": [
        "# Word pairs used to define the gender direction in Bolukbasi et al.\n",
        "female_words = ['she', 'her', 'woman', 'herself', 'daughter', 'mother',\n",
        "  'gal', 'girl', 'female']\n",
        "male_words = ['he', 'his', 'man', 'himself', 'son', 'father',\n",
        "  'guy', 'boy', 'male']\n",
        "\n",
        "# Step 1. Create a list with the word embedding differences between\n",
        "# corresponding word pairs.  For example, the first component of the list should\n",
        "# be the difference between the embeddings for 'she' and 'he'.  There are many\n",
        "# ways to do this, but the easiest is with a list comprehension based on the\n",
        "# zipped female_words and male_words.  No matter how you do it, you will need to\n",
        "# call embeddings_index.get() twice in each iteration of the loop (once for a\n",
        "# 'female' word and once for a corresponding 'male' word).\n",
        "word_diffs = [embeddings_index.get(f) - embeddings_index.get(m) for f, m in zip(female_words, male_words)]\n",
        "\n",
        "# Step 2. Call np.mean() on your list of word differences\n",
        "# You will need to specify axis = 0\n",
        "g = np.mean(word_diffs, axis = 0)\n",
        "g"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 0.14136212,  0.29730654, -0.10995344,  0.09161666, -0.20763223,\n",
              "        0.6295574 ,  0.1969412 ,  0.02930557,  0.5208999 , -0.09071961,\n",
              "        0.08637322, -0.47587886,  0.5303101 ,  0.19009914,  0.15338446,\n",
              "        0.04164857, -0.47131667,  0.03828232,  0.51945066,  0.09632424,\n",
              "        0.3348771 ,  0.47196   ,  0.18238276,  0.28140032,  0.21842666,\n",
              "        0.25748554, -0.07429378,  0.17835577, -0.11268745, -0.29692107,\n",
              "       -0.37787175,  0.20825645,  0.03342777,  0.06117866, -0.16547376,\n",
              "       -0.12674703, -0.16060212, -0.18647201,  0.16859788, -0.27167857,\n",
              "       -0.06561244, -0.24910797,  0.5795281 , -0.44391817,  0.15341921,\n",
              "       -0.28378934,  0.28439152, -0.23881333,  0.08010322, -0.02405944],\n",
              "      dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A4ztWo24f8YW",
        "colab_type": "text"
      },
      "source": [
        "#### 5. Here are the cosine similarities of several names with the gender direction vector g you obtained in part 4.  Explain what a positive and negative cosine similarity indicates, and why."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8yR83jdTed4q",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 194
        },
        "outputId": "b7f834e8-2c0c-4b2c-ca69-2e47cabb30f5"
      },
      "source": [
        "name_list = ['john', 'marie', 'sophie', 'ronaldo', 'priya', 'rahul', 'danielle',\n",
        "  'reza', 'katy', 'yasmin']\n",
        "\n",
        "for w in name_list:\n",
        "    print (w, cos_similarity(embeddings_index.get(w), g))"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "john -0.42817587\n",
            "marie 0.2553097\n",
            "sophie 0.35658687\n",
            "ronaldo -0.3242363\n",
            "priya 0.2692463\n",
            "rahul -0.15097639\n",
            "danielle 0.34718466\n",
            "reza -0.10132656\n",
            "katy 0.30913565\n",
            "yasmin 0.2528625\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lL8oz1e4geqg",
        "colab_type": "text"
      },
      "source": [
        "Since we calculated the gender direction vector as the mean of the differences of word embeddings of 'female' words minus 'male' words, a positive cosine similarity indicates a word pointing in the direction of 'female' words and a negative cosine similarity idicates a word pointing in the direction of 'male' words."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VZTOf7Ohi2il",
        "colab_type": "text"
      },
      "source": [
        "#### 6. Compute the cosine similarities of your gender direction vector g and the words 'nurse' and 'doctor'."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RfFf6c2pjxv8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "143018ca-e35f-44e3-bbd5-f9ca88fd3ed7"
      },
      "source": [
        "cos_similarity(embeddings_index.get('nurse'), g)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.2858767"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NwNysaEvj9Sn",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "1ddd3d37-b9d8-4884-fda1-a873fd5ac0b9"
      },
      "source": [
        "cos_similarity(embeddings_index.get('doctor'), g)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.003048606"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9umwoY_Akmmh",
        "colab_type": "text"
      },
      "source": [
        "#### 7. The model has learned a clear association between the word 'nurse' and gender, which we may prefer to neutralize.  Recall that this is done by taking the original word embedding and subtracting its orthogonal projection onto the gender direction vector:\n",
        "\n",
        "$$e^{corrected} = e^{original} - \\frac{g g^T}{g^T g}e^{original}$$\n",
        "\n",
        "Complete the functions below to implement the neutralize method.  Because we will need the orthogonal projection operation again below, I've pulled that step out into a separate function."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gdPi7hCEn8MN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def orth_proj(e, g):\n",
        "  '''\n",
        "  Arguments:\n",
        "   - e: a column vector\n",
        "   - g: a column vector\n",
        "  \n",
        "  Returns:\n",
        "   - the orthogonal projection of e onto the subspace spanned by g\n",
        "  '''\n",
        "  # Enforce that e and g are column vectors.  This prevents bugs that may occur\n",
        "  # if one of them is a row vector or has only one dimension (e.g. shape (50,))\n",
        "  e = e.reshape((e.shape[0], 1))\n",
        "  g = g.reshape((g.shape[0], 1))\n",
        "\n",
        "  # Calculate the orthogonal projection.\n",
        "  # You will need to call np.dot() multiple times.\n",
        "  # You can split this over multiple lines if you prefer (for example, you might\n",
        "  # calculate the projection matrix P first).\n",
        "  proj = np.dot(np.dot(g, g.T) / np.dot(g.T, g), e)\n",
        "\n",
        "  # return\n",
        "  return proj\n",
        "\n",
        "\n",
        "\n",
        "def neutralize(e, g):\n",
        "  '''\n",
        "  Neutralize the embedding e along the subspace defined by the vector g\n",
        "\n",
        "  Arguments:\n",
        "   - e: a word embedding to neutralize\n",
        "   - g: a vector defining a bias subspace\n",
        "  \n",
        "  Returns:\n",
        "   - e minus the orthogonal projection of e onto the subspace spanned by g\n",
        "  '''\n",
        "  # Enforce that e and g are column vectors.  This prevents bugs that may occur\n",
        "  # if one of them is a row vector or has only one dimension (e.g. shape (50,))\n",
        "  e = e.reshape((e.shape[0], 1))\n",
        "  g = g.reshape((g.shape[0], 1))\n",
        "\n",
        "  # Calculate neutralized result as the difference between the original vector e\n",
        "  # and its orthogonal projection onto g (you should call orth_proj).\n",
        "  e_corrected = e - orth_proj(e, g)\n",
        "\n",
        "  # return\n",
        "  return e_corrected[:,0]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Inl67tybpmFR",
        "colab_type": "text"
      },
      "source": [
        "#### 8. Call the function you defined above to neutralize the original word embedding for 'nurse'.  Calculate the cosine similarity for the updated embedding vector and the gender direction g."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zu3VV03yputS",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "f1d2f91d-df14-4953-b69a-1fce64a97d9f"
      },
      "source": [
        "e_original = embeddings_index.get('nurse')\n",
        "e_corrected = neutralize(e_original, g)\n",
        "cos_similarity(e_corrected, g)"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "9.369015e-09"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FhsrznSxptBf",
        "colab_type": "text"
      },
      "source": [
        "#### 9. Update the embedding for nurse with the corrected embedding.  Then call the `complete_analogy` function again with the updated `embeddings_index` to complete the analogy \"Man is to doctor as woman is to \\_\\_\\_\\_\\_\\_\\_.\"\n",
        "\n",
        "Note that because of how we defined the `complete_analogy` function above, it is not allowed to return 'doctor'."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7F3ikvnQNJPZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "embeddings_index['nurse'] = e_corrected"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h0U-cnr4bUbI",
        "colab_type": "code",
        "outputId": "990825da-cc4b-448e-9bcc-23dff06839b0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "complete_analogy('man', 'doctor', 'woman', embeddings_index)"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'physician'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ki8KHMNRthLp",
        "colab_type": "text"
      },
      "source": [
        "#### 10. We now turn to our second example from lecture: gender association among the words 'babysit', 'grandmother', and 'grandfather'. Compute the cosine similarity among the following vectors (you will call `cos_similarity` 3 times, once for each bullet point).\n",
        "\n",
        " * $e_{babysit}$ and the gender direction vector $g$\n",
        " * $e_{babysit}$ and $e_{grandmother}$\n",
        " * $e_{babysit}$ and $e_{grandfather}$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gxM5eGsVaejh",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "269749b9-0628-495c-c810-f3a2610d1d9f"
      },
      "source": [
        "cos_similarity(embeddings_index.get('babysit'), g)"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.2294906"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ubzB8TneqvDK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "9bd0dad2-2274-4256-f364-cb2f72268db1"
      },
      "source": [
        "cos_similarity(embeddings_index.get('babysit'), embeddings_index.get('grandmother'))"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.3096933"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "peCOdN2suHgx",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "fe2cadd1-3ee7-44c7-d278-4fc56be9fc47"
      },
      "source": [
        "cos_similarity(embeddings_index.get('babysit'), embeddings_index.get('grandfather'))"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.14852814"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ozMgVjBjiJcd",
        "colab_type": "text"
      },
      "source": [
        "From the first comparison above, we see that the model associates the word 'babysit' with the female direction in the word embedding space.  If we want to remove this association, we would do so with a neutralize step.  \n",
        "\n",
        "From the next two comparisons, we see that the model associates the word 'babysit' with both 'grandmother' and 'grandfather', but the association with 'grandmother' is stronger.  If we want to address this stronger association, we would do so with an equalize step.\n",
        "\n",
        "#### 11. Create a corrected word embedding for 'babysit' by calling `neutralize`.  Then, compute the cosine similarity among the following vectors (you will call `cos_similarity` 3 times, once for each bullet point).\n",
        "\n",
        " * $e^{corrected}_{babysit}$ and the gender direction vector $g$\n",
        " * $e^{corrected}_{babysit}$ and $e_{grandmother}$\n",
        " * $e^{corrected}_{babysit}$ and $e_{grandfather}$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yrMe0HgjuJk8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "e_corrected = neutralize(embeddings_index.get('babysit'), g)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8zhkraCLjnka",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "f6339f52-ab96-477f-d587-24b108b71e8d"
      },
      "source": [
        "cos_similarity(e_corrected, g)"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2.3395359e-08"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f6KVeQwb-v8Y",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "d1339d93-34e8-44b7-a0fe-604fc9c4398b"
      },
      "source": [
        "cos_similarity(e_corrected, embeddings_index.get('grandmother'))"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.2563601"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4u6CP-dBDvV3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "db6035ba-bb3c-43a9-cf56-ede61c8dd029"
      },
      "source": [
        "cos_similarity(e_corrected, embeddings_index.get('grandfather'))"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.19900998"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-4kc2WS7jx1Z",
        "colab_type": "text"
      },
      "source": [
        "After the neutralize step, 'babysit' is not strongly associated with the gender direction vector.  However, it is still more strongly associated with 'grandmother' than 'grandfather'.  We need to use an equalize step to address this.\n",
        "\n",
        "#### 12. Implement an equalize step.\n",
        "\n",
        "The calculations to do are listed in the equations below.  These are the equations given in Step 2a of Bolukbasi et al., but broken down into more steps."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4t7U-Hjrttnl",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "$\n",
        "\\begin{align*}\n",
        "\\mu &= \\frac{e_{1} + e_{2}}{2}\\tag{1} \\\\\n",
        "\\mu^{B} &= \\frac {g g^T}{g^T g} \\mu \\tag{2} \\\\\n",
        "\\mu^B_{\\perp} &= \\mu - \\mu^{B} \\tag{3} \\\\\n",
        "e_{1}^B &= \\frac{g g^T}{g^T g} e_{1} \\tag{4} \\\\\n",
        "e_{2}^{B} &= \\frac {g g^T}{g^T g} e_{2} \\tag{5} \\\\\n",
        "c &= \\sqrt{ |{1 - ||\\mu^B_{\\perp} ||^2} |} \\tag{6} \\\\\n",
        "e_{1}^{corrected} &= c * \\frac{e_{1}^{B} - \\mu^B} {||e_{1}^{B} - \\mu^B||} \\tag{7} \\\\\n",
        "e_{2}^{corrected} &= c * \\frac{e_{2}^{B} - \\mu^B} {||e_{1}^{B} - \\mu^B||} \\tag{8} \\\\\n",
        "e_1^{equalized} &= e_{1}^{corrected} + \\mu^B_{\\perp} \\tag{9} \\\\\n",
        "e_2^{equalized} &= e_{2}^{corrected} + \\mu^B_{\\perp} \\tag{10}\\end{align*}\n",
        "$\n",
        "\n",
        "Fill in these calculations in the equalize function below."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EevTmX1Ett6K",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def equalize(e1, e2, g):\n",
        "  '''\n",
        "  Debias gender specific words by following the equalize method described in\n",
        "  Bolukbasi et al.\n",
        "  \n",
        "  Arguments:\n",
        "    - e1: first word embedding to equalize\n",
        "    - e2: second word embedding to equalize\n",
        "    - g: a vector defining a bias subspace\n",
        "  \n",
        "  Returns:\n",
        "    - e1_equalized, e2_equalized: equalized versions of e1 and e2\n",
        "  '''\n",
        "  # Enforce that e and g are column vectors.  This prevents bugs that may occur\n",
        "  # if one of them is a row vector or has only one dimension (e.g. shape (50,))\n",
        "  e1 = e1.reshape((e1.shape[0], 1))\n",
        "  e2 = e2.reshape((e2.shape[0], 1))\n",
        "  g = g.reshape((g.shape[0], 1))\n",
        "\n",
        "  # Step 1: Compute the mean of e1 and e2\n",
        "  # (you could just add them up and divide by 2)\n",
        "  mu = (e1 + e2) / 2\n",
        "\n",
        "  # Step 2: Compute the orthogonal projection of mu onto the bias subspace\n",
        "  # using your orth_proj function from above\n",
        "  mu_B = orth_proj(mu, g)\n",
        "\n",
        "  # Step 3: Compute the component of mu that's orthogonal to the bias subspace\n",
        "  mu_orth = mu - mu_B\n",
        "\n",
        "  # Steps 4 and 5: Compute e1_B and e2_B by calling orth_proj\n",
        "  e1_B = orth_proj(e1, g)\n",
        "  e2_B = orth_proj(e2, g)\n",
        "  \n",
        "  # Step 6: Calculate the coefficient for the equalized embeddings\n",
        "  c = np.sqrt(np.abs(1. - np.dot(mu_orth.T, mu_orth)))\n",
        "\n",
        "  # Steps 7 and 8: Calculate the corrected embeddings, minus the orthogonal component\n",
        "  corrected_e1_B = c * (e1_B - mu_B) / np.linalg.norm(e1_B - mu_B)\n",
        "  corrected_e2_B = c * (e2_B - mu_B) / np.linalg.norm(e2_B - mu_B)\n",
        "\n",
        "  # Steps 9 and 10: Obtain the final equalized embeddings by adding the orthogonal component\n",
        "  e1_equalized = corrected_e1_B + mu_orth\n",
        "  e2_equalized = corrected_e2_B + mu_orth\n",
        "\n",
        "  # return\n",
        "  return e1_equalized[:, 0], e2_equalized[:, 0]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ps-caKl-mwVd",
        "colab_type": "text"
      },
      "source": [
        "#### 13. Obtain the equalized embeddings for 'grandmother' and 'grandfather'.  Then calculate the cosine similarities between the following embeddings (one call to `cos_similarity` for each bullet point):\n",
        " * `e_corrected` (the neutralized embedding of 'babysit' from part 11) and `e_gmother_equalized`\n",
        " * `e_corrected` (the neutralized embedding of 'babysit' from part 11) and `e_gfather_equalized`\n",
        " * `g` and `e_gmother_equalized`\n",
        " * `g` and `e_gfather_equalized`\n",
        "\n",
        " "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2P0c_MbWDyV6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "e_gmother_equalized, e_gfather_equalized = equalize(embeddings_index.get('grandmother'), embeddings_index.get('grandfather'), g)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1gINz6-UNKxu",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "2fd6138f-8310-4581-c8c5-67a37936e722"
      },
      "source": [
        "cos_similarity(e_corrected, e_gmother_equalized)"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.17213492"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6-DfwODINQ4L",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "d9e680da-7327-484a-d370-c7acc9e0befe"
      },
      "source": [
        "cos_similarity(e_corrected, e_gfather_equalized)"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.1721349"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qjth4pJ6nccZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "eb3a3614-139f-4861-ced0-3f0510d4a89b"
      },
      "source": [
        "cos_similarity(g, e_gmother_equalized)"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.6975457"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lNSPIduQnd7i",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "f1e421a6-95ba-42fa-a685-1fa2bfd683f2"
      },
      "source": [
        "cos_similarity(g, e_gfather_equalized)"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "-0.69754577"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 37
        }
      ]
    }
  ]
}